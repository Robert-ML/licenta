{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "\n",
    "https://answers.opencv.org/question/194679/detecting-embossed-texts-characterstechniques/\n",
    "\n",
    "https://pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import contours\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the reference OCR-A image from disk, convert it to grayscale, and threshold it, such that the digits appear as *white* on a *black* background and invert it, such that the digits appear as *white* on a *black*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = cv2.imread(\"./in/ocr-b-font.png\")\n",
    "ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find contours in the OCR-A image (i.e,. the outlines of the digits) sort them from left to right, and initialize a dictionary to map digit name to the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "refCnts = imutils.grab_contours(refCnts)\n",
    "refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
    "characters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should loop through the contours, extract, and associate ROIs with their corresponding characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, c) in enumerate(refCnts):\n",
    "\t# compute the bounding box for the digit, extract it, and resize\n",
    "\t# it to a fixed size\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\troi = ref[y:y + h, x:x + w]\n",
    "\troi = cv2.resize(roi, (57, 88))\n",
    "\t# update the characters dictionary, mapping the digit name to the ROI\n",
    "\tcharacters[i] = roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are done extracting the digits from our reference image and associating them with their corresponding digit name.\n",
    "\n",
    "Let’s continue by initializing a couple structuring kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a rectangular (wider than it is tall) and square\n",
    "# structuring kernel\n",
    "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))\n",
    "sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s prepare the image we are going to OCR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./in/card_input.jpg\")\n",
    "image = imutils.resize(image, width=300)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(\"./out/embrossed_text/gray.jpg\", gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our image is grayscaled and the size is consistent, let’s perform a morphological operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a tophat (whitehat) morphological operator to find light\n",
    "# regions against a dark background (i.e., the credit card numbers)\n",
    "tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)\n",
    "cv2.imwrite(\"./out/embrossed_text/tophat.jpg\", tophat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our tophat image, let’s compute the gradient along the x-direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Scharr gradient of the tophat image, then scale\n",
    "# the rest back into the range [0, 255]\n",
    "gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0,\n",
    "\tksize=-1)\n",
    "gradX = np.absolute(gradX)\n",
    "(minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
    "gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))\n",
    "gradX = gradX.astype(\"uint8\")\n",
    "cv2.imwrite(\"./out/embrossed_text/gradX.jpg\", gradX)\n",
    "# Computing the Scharr gradient magnitude representation of the image reveals vertical changes in the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s continue to improve our credit card digit finding algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a closing operation using the rectangular kernel to help\n",
    "# cloes gaps in between credit card number digits, then apply\n",
    "# Otsu's thresholding method to binarize the image\n",
    "gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)\n",
    "cv2.imwrite(\"./out/embrossed_text/gradX_close.jpg\", gradX)\n",
    "thresh = cv2.threshold(gradX, 0, 255,\n",
    "\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "cv2.imwrite(\"./out/embrossed_text/thresh.jpg\", thresh)\n",
    "# apply a second closing operation to the binary image, again\n",
    "# to help close gaps between credit card number regions\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "cv2.imwrite(\"./out/embrossed_text/thresh_close.jpg\", thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let’s find the contours and initialize the list of character grouping locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours in the thresholded image, then initialize the\n",
    "# list of digit locations\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s loop through the contours while filtering based on the aspect ratio of each, allowing us to prune the digit group locations from other, irrelevant areas of the credit card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = []\n",
    "# loop over the contours\n",
    "for (i, c) in enumerate(cnts):\n",
    "\t# compute the bounding box of the contour, then use the\n",
    "\t# bounding box coordinates to derive the aspect ratio\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\tar = w / float(h)\n",
    "\t# since credit cards used a fixed size fonts with 4 groups\n",
    "\t# of 4 digits, we can prune potential contours based on the\n",
    "\t# aspect ratio\n",
    "\tif ar > 2.5 and ar < 4.0:\n",
    "\t\t# contours can further be pruned on minimum/maximum width\n",
    "\t\t# and height\n",
    "\t\tif (w > 40 and w < 55) and (h > 10 and h < 20):\n",
    "\t\t\t# append the bounding box region of the digits group\n",
    "\t\t\t# to our locations list\n",
    "\t\t\tlocs.append((x, y, w, h))\n",
    "\n",
    "print(\"locs:\", len(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll sort the groupings from left to right and initialize a list for the credit card digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the digit locations from left-to-right, then initialize the\n",
    "# list of classified digits\n",
    "locs = sorted(locs, key=lambda x:x[0])\n",
    "output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know where each group of four digits is, let’s loop through the four sorted groupings and determine the digits therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the 4 groupings of 4 digits\n",
    "for (i, (gX, gY, gW, gH)) in enumerate(locs):\n",
    "\t# initialize the list of group digits\n",
    "\tgroupOutput = []\n",
    "\t# extract the group ROI of 4 digits from the grayscale image,\n",
    "\t# then apply thresholding to segment the digits from the\n",
    "\t# background of the credit card\n",
    "\tgroup = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\n",
    "\tgroup = cv2.threshold(group, 0, 255,\n",
    "\t\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\t# detect the contours of each individual digit in the group,\n",
    "\t# then sort the digit contours from left to right\n",
    "\tdigitCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "\tdigitCnts = imutils.grab_contours(digitCnts)\n",
    "\tdigitCnts = contours.sort_contours(digitCnts,\n",
    "\t\tmethod=\"left-to-right\")[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

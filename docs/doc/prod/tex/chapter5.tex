\chapter{Evaluation}\pagestyle{fancy}\setlength{\parindent}{3em}
\label{chapter:evaluation}

As I developed a processing pipeline to extract the tire-markings off images of tires, I needed a dataset. To build this dataset I took photos with a Cannon EOS 1300D that has a resolution of 5184 by 3456 pixels, but the picture was cropped around the center to 3456 by 3456 pixels so that the image would be square. The lens used is EF-S 18-55mm Canon Zoom which has diameter of 58mm. When taking pictures I used a focal length of 23 millimeters and the aperture, exposure time and ISO were kept on "auto". In the process of building the dataset, the distance from the lens to the tire's side was approximately (TODO: distanta aproximativa pana la cauciuc cand fac poze). The dataset size is of 107 images.

While capturing the images, I was careful to catch the entire wheel in the image because detecting half wheels or arcs would have proven difficult. One more adjustment I did was the camera placement in regards to the wheel's axle. I decided to be approximately in line with it in order for the wheel to appear circular in the image. If I wouldn't have done so, the tire would have had an oval shape which would not affect very much the unwrapping faze as it has some slack and it doesn't require the tire to be perfectly circular. Anyway, the outer rim of the tire doesn't have a circular shape because it is deformed at the contact point with the ground. The oval shape affects the unwrapped output like Figure (TODO: cauciuc care nu i perfect orizontal), compared to Figure (TODO: imagine cu un cauciuc unwrapped frumos).

By controlling the distance between the camera and the wheel, as  well as the camera position in regards to the wheel's axle, I was be able to detect the tire in the image more reliably and ensured the letters composing the tire-markings had at least 20 pixels in height.

When it comes to evaluating the running time, the system the algorithm is tested on contains an Intel i7-6700HQ CPU and 16GB of DDR4 RAM at 2133 Mega-transfers/s. The operating system is Ubuntu 20.04.4 LTS 64 bit with Linux Kernel 5.5.9-050509-generic. In measuring the time, I do not take into account the writing time to the storage media.

There are three main points in which the pipeline can be evaluated and they coincide with its steps. The first is the percentage of the dataset in which the tire could be identified and unwrapped. The second is the percentage of codes found in average on a tire and the average quantity of false positives in the regions of interest extracted. The third is the performance of the OCR in correctly recognizing the regions of interest.

\section{Evaluating the Tire Unwrapping (Stage 1)}\label{section:evaluation-tire_unwrapping}

I measured what percentage of the input images my algorithm is able to unwrap successfully. This is partially automated and partially manual. My algorithm tries to obtain the circles approximately matching to the outer and inner rims of the tire. If more circles are detected and the algorithm is not able to reduce their number to only two or not even 2 are found, the respective image will be discarded automatically. The manual part is to pass through each successfully unwrapped image and check if the wheel's center was correctly detected. If not, I will discard the image before proceeding to the next phase.

It turned out that the pipeline could identify and unwrap the tire in 86 images out of the 107 in the dataset. Out of these 86 unwrapped images, 5 were false positives that I observed while manually passing through them. So the first stage of the algorithm could find and unwrap the tire in 75.7\% of the dataset.

By performing the action of unwrapping and keeping only the tire, I succeeded in reducing the number of pixels in the image by an average of 7,139,770 pixels (59,77\% of the squared original size), the distribution of the pixel reduction count could be seen in Figure \ref{fig:pixel_count_reduced-stage1}.

This stage on the hardware configuration previously stated has an average running time of 1.39 seconds (Figure \ref{fig:running_time-stage1}). It must be noted that the IO operations with the long term storage medium are not taken into account when it comes to measuring the running time.

\begin{figure}
    \centering
    \begin{minipage}[c]{0.5\linewidth}
        \centering
        \includegraphics[height=7cm, keepaspectratio]{img/evaluation/no_of_pixels_reduced-stage1.pdf}
            \caption{No. of pixels reduced by Stage 1}
            \label{fig:pixel_count_reduced-stage1}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.5\linewidth}
        \centering
        % TODO: figure
        \includegraphics[height=7cm, keepaspectratio]{img/evaluation/running_time-stage1.pdf}
            \caption{Running time of Stage 1}
            \label{fig:running_time-stage1}
    \end{minipage}
\end{figure}

\section{Evaluating the Text Detection (Stage 2)}\label{section:evaluation-text_detection}

Here the main metric will be the percentage of images the algorithm is able to detect the DOT code and the E-mark certification successfully. Other markings on the tire that are recognized are taken also into account as true positives.

Before measuring the performance, it is needed to define a few edge cases. If the text of a marking (for example the DOT code) is split in multiple segments instead of single one, it will be considered as a valid recognition and a new metric for segmented detections will be used. If a text region is incomplete recognized, it will be considered as being valid, but it will account to another metric for incomplete recognitions. If the past 2 cases combine, it will be considered the tire-marking is also segmented and incomplete. For example if there is the DOT code "DOT ABCD EFG 0123" and it was detected as 2 regions "DOT ABCD" and "01", they will be considered as one valid recognition, one segmented recognition and one incomplete recognition. The regions that do not contain text are considered false positives.

Other metrics that I measured is the average time it takes to run the text detection algorithm and the dimension reduction of the image by identifying the regions of interest.

TODO: plot the results. (pot face chestia aia cu patratul cu distributia si sa bag si tabel)

\section{Evaluating the Text Recognition (Stage 3)}\label{section:evaluation-ocr}

When it comes to evaluating the OCR, I will use the Character Error Rate (CER) \cite{site:evaluation-OCR-character_error_rate} metric. This will show the performance of the algorithm in recognizing the letters out of the good regions of interest. I will hand pick the true positive images containing text and discard the false positives detected at the \ref{sec:text-detection} Text Detection section step because I have limited a limited number of credits to try out the OCR (called Read API) from Microsoft Cognitive Services \cite{site:Microsoft_Cognitive_Services}.

Measuring the running time is not a precise task as the OCR is a web service and it would depend on the network congestion and the service's load at the time. Anyhow, I still measured the time it takes from the submission of the request to the response.

TODO: Evaluate and talk about the results.

\section{Overall Evaluation}\label{section:evaluation-overall}

TODO: talk about overall results and compare mine with the state of the art.

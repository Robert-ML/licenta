\chapter{State of the Art}\pagestyle{fancy}\setlength{\parindent}{3em}
\label{chap:state-of-the-art}

In designing my solution I've taken into account other work performed in the field of automatically recognizing tire-markings. The difficulty of the task is twofold. On one hand, the markings on tires' sides are rarely printed because they would fade in time or under the effects of the elements. Instead, the approach employed by manufacturers is to have embossed letters on the side of the tire that would fade slower than their painted counter parts. They are not foolproof either as they can also fade in time as the letters dull and become indistinguishable from the normal side of the tire, but are usually enough to outlive the tire grooves that dull the first because of the tire's usage. One characteristic of these embossed letters is that they are practically rubber on rubber, so black on black (Figure 4a). Their visibility is not great, even with the human eye can be hard to distinguish the letters in some lighting conditions. For the human eye, it can be beneficial to have an incident source of light that would make the letters cast a shadow on the side of the tire like can be seen in Figure 4b and a person could accomplish this with a flashlight that he positions at an optimal angle.

TODO: Figure 4 (a and b)

This was also the approach of Wajahat Kazmi et alia in their work \cite{article:1}. They had a 2 camera setup that each would capture half of the wheel and a supplementary source of “Strobe light incident at steep angles with respect to the plane of the sidewall” to quote them. This arrangement would help with the resolution of the images and character detection in the later stages of their pipeline. As a first step, they also performed an unwrapping of the tire using Circular Hough Transform \cite{site:circular_hough_transform}. After, they focused on detecting only the DOT code by using their crafted features in order to keep a low memory footprint (the average sizes of their images were 500x2800 pixels) and extracted a Histogram of Oriented Gradients. This output they would feed in a Convolutional Neural Network based Multi-Layered Perceptron and would have as output regions of the image where text is present. The training was done with a synthetic data-set. Then, they would localize in the proposed areas the DOT code by using a deep neural network trained on a synthetic data-set of DOT foregrounds and different tire backgrounds. The same model was used also for character recognition and was trained on a 700,000 synthetic data-set of characters on black background to mimic the low contrast appearance of the embossed markings. They claim their pipeline obtained an accuracy of 80\% in images that are considered acceptable to human standards, but make a note that an objective benchmark is not available. The lower quality images that would pose difficulties even to a human to recognize the text obtained an accuracy raging from 73\% to until 14\%. TODO: sa spun aici de ce eficienta am eu si ca data-set-ul il pot face public ca lumea sa faca banchmark pe el.

While the past paper's goal was for an industrial system that would have controlled conditions when performing the tire-markings recognition, there is also work in the field, by Anton Katanaev et alia \cite{site:0} for a consumer solution that would try to extract the tire specifications in order to help with ordering new ones. Their goal is to obtain the “ISO metric tire codes” that specify the type of tire, the width, aspect ratio, construction, diameter, load index and speed rating. Their first step was to collect a data-set of tire images by using internet scrappers and then filter through them using a classification model based off ResNet64. Compared to the previous work, Anton Katanaev et alia found the Circular Hough Transform unsuitable because of parameter tuning and opted for a segmentation approach for detecting the tire from the background. After this, they focused on image preprocessing to combat the different illumination in the images collected in the data-set and also perform circle correction for when the tires were appearing with an oval shape because of the camera angle. To reduce the space in which to perform the character recognition, the team also employed a step in which regions of text are searched in the image. They compared 14 pre-trained text detection models and chose the best performing one to provide supposed regions of text. After this, the proposed regions are fed into the best performing text recognition model they've tried: SEG OCR. This model obtained a character error rate of 0.16 on the original images, performing worst on the ones where the contrast was adjusted. As a final step to combat errors that might appear in the character recognition part and to identify a tire by its properties, they used a database of more than 15,000 tire characteristics for correcting incomplete or erroneous data. Because of the many learning models used, the processing part of their application is in cloud (they leverage the computational power of AWS Cloud). On the user's end is a smartphone application to submit the captured tire image to the server to start processing and provide suitable tire candidates.

In our \hyperref[sec:problem-statement]{Problem Statement} we stated the desire of a system that would not require complex setups, a controlled light environment and that would be nice to be as much as possible self contained (to not rely completely on a server for processing). This is why in my approach I preferred mathematical algorithms over learning models to accomplish the task and provide a robust system that is can perform in natural lighting conditions. TODO: sa introduc chestii despre acuratete dupa ce o calculez